{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM9QVnfN8POtBtSzRnrd31",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mervegb/deep-learning/blob/main/spam_or_ham.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Steps\n",
        "- Preprocessing => tokenize the text and prepare the data for training\n",
        "- Word Embeddings => use pre-trained word embeddings\n",
        "- Model => create a simple model with a linear layer\n",
        "- Training => train the model\n",
        "- Evaluation => evaluate the model performance"
      ],
      "metadata": {
        "id": "vAEvj2DgxyJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 0 is ham, 1 is spam\n",
        "data = [(\"Buy two get one free\", 1),\n",
        "        (\"Call you later\", 0),\n",
        "        (\"Congratulations, you won\", 1),\n",
        "        (\"Meeting at 3 PM\", 0),\n",
        "        (\"Click this link\", 1),\n",
        "        (\"How are you?\", 0),\n",
        "        (\"You have won a prize\", 1),\n",
        "        (\"Dinner at 7?\", 0),\n",
        "        (\"URGENT! Your account is compromised\", 1),\n",
        "        (\"Looking forward to seeing you\", 0),\n",
        "        (\"Earn money fast\", 1),\n",
        "        (\"Can we reschedule?\", 0)]"
      ],
      "metadata": {
        "id": "bDJERPT_sEdR"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vocabulary\n",
        "vocab = set(word for text, _ in data for word in text.split())\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n"
      ],
      "metadata": {
        "id": "fl9-uLCeqGNx"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to numbers (Tokenization)\n",
        "X = [[word_to_index[word] for word in text.split()] for text, _ in data]\n",
        "y = [label for _, label in data]"
      ],
      "metadata": {
        "id": "vTqx2WkIqTSv"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class SpamHamClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size):\n",
        "    super(SpamHamClassifier, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,embed_size)\n",
        "    self.linear = nn.Linear(embed_size,2)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.embedding(x).mean(dim=1)\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "Sbguhxz9rNog"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "model = SpamHamClassifier(vocab_size,10)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "20Z9O4jhsCXn"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the maximum sentence length\n",
        "max_len = max(len(x) for x in X)\n",
        "\n",
        "# Pad the shorter sentences with zeros\n",
        "X_padded = [x + [0]*(max_len - len(x)) for x in X]\n",
        "\n",
        "# Convert padded data to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_padded, dtype=torch.long)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)"
      ],
      "metadata": {
        "id": "LuQp-uSIv3Q4"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding\n",
        "Padding is a technique used in data preprocessing to make sequences have the same length. This is particulary important when you're feeding the data into models like neural networks that require fixed-size input.\n",
        "\n",
        "Neural networks often expect input of a constant size. In this scenario, you'd pad the shorter sentences with extra 'empty' values (often zeros) to match the longest one:\n",
        "\n",
        "\"I love AI.\" -> \"I love AI. PAD PAD PAD PAD PAD\"\n",
        "\"I'm a junior to mid software developer.\" -> \"I'm a junior to mid software developer.\"\n",
        "\"I want to switch careers.\" -> \"I want to switch careers. PAD PAD PAD\""
      ],
      "metadata": {
        "id": "W6nUR0MAySuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_tensor)\n",
        "\n",
        "    loss = loss_fn(output, y_tensor)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIb9Mu26sNkO",
        "outputId": "5b1e0a4c-bd72-46df-f852-28b797e35c60"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0006097842706367373\n",
            "Epoch 2, Loss: 0.000609248352702707\n",
            "Epoch 3, Loss: 0.0006087521323934197\n",
            "Epoch 4, Loss: 0.0006082063191570342\n",
            "Epoch 5, Loss: 0.0006076901918277144\n",
            "Epoch 6, Loss: 0.0006071741809137166\n",
            "Epoch 7, Loss: 0.0006066581117920578\n",
            "Epoch 8, Loss: 0.0006061222520656884\n",
            "Epoch 9, Loss: 0.0006056160200387239\n",
            "Epoch 10, Loss: 0.0006050899974070489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    test_text = \"You won a gift from us!\"\n",
        "\n",
        "    # Tokenize and pad the test sentence\n",
        "    test_token = [word_to_index.get(word, 0) for word in test_text.split()]\n",
        "    test_token_padded = test_token + [0] * (max_len - len(test_token))\n",
        "\n",
        "    # Convert to a tensor\n",
        "    test_tensor = torch.tensor([test_token_padded], dtype=torch.long)\n",
        "\n",
        "    # Run the model\n",
        "    output = model(test_tensor)\n",
        "\n",
        "    # Get the prediction\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(\"Spam\" if predicted.item() == 1 else \"Ham\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7BuIHJQt0aA",
        "outputId": "69c739e3-53f7-49ba-9f1e-b329b5bc8904"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spam\n"
          ]
        }
      ]
    }
  ]
}